[
{
	"uri": "/_header/",
	"title": "header",
	"tags": [],
	"description": "",
	"content": "Flokkr documentation\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/launcher/envtoconf/",
	"title": "Configuration transformer plugin",
	"tags": [],
	"description": "",
	"content": " Could be activated by CONFIG_TYPE=simple settings, but it\u0026rsquo;s the default.\nEvery configuration could be defined with environment variables, and they will be converted finally to hadoop xml, properties, conf or other format. The destination format (and the destination file name) is defined with the name of the environment variable according to a naming convention.\nThe generated files will be saved to the $CONF_DIR directory.\nThe source code of the converter utility can be found in a separated repository.\nNaming convention for set config keys from enviroment variables To set any configuration variable you shold follow the following pattern:\nNAME.EXTENSION_configkey=VALUE  The extension could be any extension which has a predefined transformation (currently xml, yaml, properties, configuration, yaml, env, sh, conf, cfg)\nexamples:\nCORE-SITE_fs.default.name: \u0026quot;hdfs://localhost:9000\u0026quot; HDFS-SITE_dfs_namenode_rpc-address: \u0026quot;localhost:9000\u0026quot; HBASE-SITE.XML_hbase_zookeeper_quorum: \u0026quot;localhost\u0026quot;  In some rare cases the transformation and the extension should be different. For example the kafka server.properties should be in the format key=value which is the cfg transformation in our system. In that case you can postfix the extension with an additional format specifier:\nNAME.EXTENSION!FORMAT_configkey=VALUE  For example:\nSERVER.CONF!CFG_zookeeper.address=zookeeper:2181  Available transformation  xml: HADOOP xml file format\n properties: key value pairs with : as separator\n cfg: key value pairs with = as separator\n conf: key value pairs with space as spearator (spark-defaults is an example)\n env: key value paris with = as separator\n sh: as the env but also includes the export keyword\n##### Configuration reference\nThe plugin itself could be configured with the following environment variables.\n  | Name | Default | Description | | \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- | | CONF_DIR | Set in the docker container definitions | The location where the configuration files will be saved. | | CONFIG_TYPE | simple | For compatibility reason. If the value is simple, the conversion is active. |\n"
},
{
	"uri": "/launcher/consul/",
	"title": "Consul config loading",
	"tags": [],
	"description": "",
	"content": " Could be activated with CONFIG_TYPE=consul\n The starter script list the configuration file names based on a consul key prefix. All the files will be downloaded from the consul key value store and the application process will be started with consul-template (enable an automatic restart in case of configuration file change)  The source code of the consul based configuration loading and launcher is available at the elek/consul-launcher repository.\n   Name Default Description     CONF_DIR Set in the docker container definitions The location where the configuration files will be saved.   CONFIG_TYPE consul For compatibility reason. If the value is consul, the consul based configuration handling is active.   CONSUL_PATH conf The path of the subtree in the consul where the configurations are.   CONSUL_KEY  The path where the configuration for this container should be downloaded from. The effective path will be $CONSUL_PATH/$CONSUL_KEY    BTRACE: btrace instrumentation Could be enabled with setting BTRACE_ENABLED=true or just setting BTRACE_SCRIPT.\nIt adds btrace javaagent configuration to the JAVA_OPTS (or any other opts defined by BTRACE_OPTS_VAR). The standard output is redirected to /tmp/output.log, and the btrace output will be displayed on the standard output (over a /tmp/btrace.out file)\n   Name Default Description     CONF_DIR Set in the docker container definitions The location where the configuration files will be saved.   BTRACE_SCRIPT  The location of the compiled btrace script. Coule be absolute or relative to the /opt/plugins/020_btrace/btrace   BTRACE_OPTS_VAR JAVA_OPTS The name of the shell variable where the agent parameters should be injected.    Configuration  CONSUL_PATH defines the root of the subtree where the configuration are downloaded from. The root could also contain a configuration config.ini. Default is conf\n CONSUL_KEY is optional. It defines a subdirectory to download the the config files. If both CONSUL_PATH and CONSUL_KEY are defined, the config files will be downloaded from $CONSUL_PATH/$CONSUL_KEY but the config file will be read from $CONSUL_PATH/config.ini\n  "
},
{
	"uri": "/",
	"title": "Docker images for Open Source bigdata/hadoop projects",
	"tags": [],
	"description": "",
	"content": " Flokkr project provides a way to run Apache bigdata applications (such as Apache Hadoop, Spark or Ozone) in Kubernetes.\nIt provides\n Container images with the latest version of the projects (docker-* git repositories) Kubernetes resource definitions based on flekszible Base image Launcher script to handle various use cases (kerberos keytab generation, configuration management, etc)  Kubernetes deploy There is No One Size Fits All.\nYou need specific setup for each kubernetes environment. Helm charts are definitely not flexible enough the diversity of configuration options and can\u0026rsquo;t handle the dependencies between different projects.\nTherefore flokkr is based on a composition based kubernetes configuration manager: (https://github.com/elek/flekszible)\nFor more information read the Kubernetes page.\nContainers All of the containers are based on one smart baseimage defined in flokkr/docker-baseimage. It contains all the configuration loading script (based on environment variables or consul servers) and other extensions (eg. btrace instrumentation).\nTo get more information about the available environment variables check the flokkr/launcher repository.\nAll the other containers can be found with docker- prefix under the flokkr organization.\nThe containers are usually built on travis-ci and pushed to the docker hub instead to use dockerhub automatic buidls due to the limitation of the dockerhub (for example it\u0026rsquo;s hard to generate matrix builds with all the older versions).\nAvailable images:\n   Repository Product     docker-baseimage Base image with all the configuration loading magic   docker-hadoop Apache Hadoop components (hdfs/yarn)   docker-spark Apache Spark components   docker-zookeeper Apache Zookeeper components   docker-kafka Apache Kafka components   docker-hbase Apache HBase components   docker-zeppelin Apache Zeppelin interface   docker-krb5 Highly insecure kerberos container, with an open REST api to request new kerberos keytab files.    Note: previous version of the containers (and some not yet migrated) can be found under the github.com/elek account.\n"
},
{
	"uri": "/launcher/",
	"title": "Flokkr launcher",
	"tags": [],
	"description": "",
	"content": "The base docker images (and after that every docker image) contains a simple script to launch the command (docker entrypoint).\nThe source of this launcher script is maintained in the launcher repository.\nThe launcher scripts countains multiple conditional script fragment which could be activated by environment variables. These fragments provide additional functionality to the base image. For example:\n Could convert environment variables to configuration files Could instrument java applcation with btrace agent Could retry the execution Could read configuration from consul server Could sleep for a specific time  "
},
{
	"uri": "/launcher/installer/",
	"title": "Installer plugin",
	"tags": [],
	"description": "",
	"content": "Installer plugin could replace built in components\nThe original products usually unpacked to the /opt directory during the container build (eg. /opt/hadoop, /opt/spark, etc\u0026hellip;). The install plugin deletes the original product directory and replaces it with a newly one downloaded from the internet.\n   Name Default Description     INSTALLER_XXX  The value of the environment variable should be an url. If set, the URL will be downloaded and untar-ed to the /opt/xxx directory. For example set INSTALER_HADOOP=http://home.apache.org/~shv/hadoop-2.7.4-RC0/https://home.apache.org/~shv/hadoop-2.7.4-RC0/hadoop-2.7.4-RC0.tar.gz to test an RC.    "
},
{
	"uri": "/launcher/kerberos/",
	"title": "Kerberos plugin",
	"tags": [],
	"description": "",
	"content": " Kerberos plugin downloads/generates kerberos keytabs and ssl key/truststore\nOur total UNSECURE kerberos server contains a REST endpoint to download on-the-fly generated kerberos keytabs, java keystores (ssl keystores, trustores). This plugin could be configured to download the files. The plugin also copies krb5.cfg to /etc.\nConfiguration    Name Default Description     KERBEROS_SERVER krb5 The name of the UNSECURE kerberos server where the REST endpoint is available on :8081   KERBEROS_KEYTABS  Space separated list of keytab names. With every element a new keytab will generated to $CONF_DIR/$NAME.keytab with a key for $NAME/$HOSTNAME@EXAMPLE.COM.   KERBEROS_KEYSTORES  Space separated list of certificate names. For every name a new keystore file will be generated to the $CONF_DIR/$NAME.keystore which contains a key for cn=$NAME. Trust store will also be generated to $CONF_DIR/truststore.    "
},
{
	"uri": "/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": "Flokkr is based on a composition based kubernetes resource manager flekszible.\nWith this approach you can use different configuration for different clusters.\nhttps://github.com/flokkr/k8s repository contains the configuration set which can be combined in any way and customized. See the ./examples sub-directory for examples.\nUsually you need the following:\n install flekszible Create a new directory and import required components from your flekszible.yaml Generate kubernetes resources with flekszible generate Use the final kubernetes resources as is  "
},
{
	"uri": "/launcher/retry/",
	"title": "Retry plugin",
	"tags": [],
	"description": "",
	"content": " The plugin tries to run the entrypoint of the image multiple times. If the process has been exited with non zero exit code, it tries to rerun the command after a sleep. The sleep time increasing with every iteration and the whole process will be stopped anyway after a fix amount of retry. If the process run enough time (60s) the failure counter and sleep time is reseted.\nConfiguration    Name Default Description     RETRY_NUMBER 10 Number of times the process will be restarted (in case of non-zero exit code   RETRY_NORMAL_RUN_DURATION 60 After this amount of seconds the RETRY_NUMBER counter will be reseted. Example: After 5 tries the process is started and run successfully 5 minutes. After a non-zer exit, it will be rerun RETRY_NUM (10) times. Example 2: After 5 tries the process is starts, runs for 40 seconds, and exits. The retry will continue with the reamining 5 try.    "
},
{
	"uri": "/launcher/sleep/",
	"title": "Sleep plugin",
	"tags": [],
	"description": "",
	"content": " SLEEP: sleep for a specified amount of time.    Name Default Description     SLEEP_SECONDS  If set, the sleep bash command will be called with the value of the environment variable. Better to not use this plugin, if possible.    "
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]