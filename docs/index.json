[
{
	"uri": "https://flokkr.github.io/docs/_header/",
	"title": "header",
	"tags": [],
	"description": "",
	"content": "Flokkr documentation\n"
},
{
	"uri": "https://flokkr.github.io/docs/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://flokkr.github.io/docs/launcher/envtoconf/",
	"title": "Configuration transformer plugin",
	"tags": [],
	"description": "",
	"content": " Could be activated by CONFIG_TYPE=simple settings, but it\u0026rsquo;s the default.\nEvery configuration could be defined with environment variables, and they will be converted finally to hadoop xml, properties, conf or other format. The destination format (and the destination file name) is defined with the name of the environment variable according to a naming convention.\nThe generated files will be saved to the $CONF_DIR directory.\nThe source code of the converter utility can be found in a separated repository.\nNaming convention for set config keys from enviroment variables To set any configuration variable you shold follow the following pattern:\nNAME.EXTENSION_configkey=VALUE  The extension could be any extension which has a predefined transformation (currently xml, yaml, properties, configuration, yaml, env, sh, conf, cfg)\nexamples:\nCORE-SITE_fs.default.name: \u0026quot;hdfs://localhost:9000\u0026quot; HDFS-SITE_dfs_namenode_rpc-address: \u0026quot;localhost:9000\u0026quot; HBASE-SITE.XML_hbase_zookeeper_quorum: \u0026quot;localhost\u0026quot;  In some rare cases the transformation and the extension should be different. For example the kafka server.properties should be in the format key=value which is the cfg transformation in our system. In that case you can postfix the extension with an additional format specifier:\nNAME.EXTENSION!FORMAT_configkey=VALUE  For example:\nSERVER.CONF!CFG_zookeeper.address=zookeeper:2181  Available transformation  xml: HADOOP xml file format\n properties: key value pairs with : as separator\n cfg: key value pairs with = as separator\n conf: key value pairs with space as spearator (spark-defaults is an example)\n env: key value paris with = as separator\n sh: as the env but also includes the export keyword\n##### Configuration reference\nThe plugin itself could be configured with the following environment variables.\n  | Name | Default | Description | | \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- | | CONF_DIR | Set in the docker container definitions | The location where the configuration files will be saved. | | CONFIG_TYPE | simple | For compatibility reason. If the value is simple, the conversion is active. |\n"
},
{
	"uri": "https://flokkr.github.io/docs/launcher/consul/",
	"title": "Consul config loading",
	"tags": [],
	"description": "",
	"content": " Could be activated with CONFIG_TYPE=consul\n The starter script list the configuration file names based on a consul key prefix. All the files will be downloaded from the consul key value store and the application process will be started with consul-template (enable an automatic restart in case of configuration file change)  The source code of the consul based configuration loading and launcher is available at the elek/consul-launcher repository.\n   Name Default Description     CONF_DIR Set in the docker container definitions The location where the configuration files will be saved.   CONFIG_TYPE consul For compatibility reason. If the value is consul, the consul based configuration handling is active.   CONSUL_PATH conf The path of the subtree in the consul where the configurations are.   CONSUL_KEY  The path where the configuration for this container should be downloaded from. The effective path will be $CONSUL_PATH/$CONSUL_KEY    BTRACE: btrace instrumentation Could be enabled with setting BTRACE_ENABLED=true or just setting BTRACE_SCRIPT.\nIt adds btrace javaagent configuration to the JAVA_OPTS (or any other opts defined by BTRACE_OPTS_VAR). The standard output is redirected to /tmp/output.log, and the btrace output will be displayed on the standard output (over a /tmp/btrace.out file)\n   Name Default Description     CONF_DIR Set in the docker container definitions The location where the configuration files will be saved.   BTRACE_SCRIPT  The location of the compiled btrace script. Coule be absolute or relative to the /opt/plugins/020_btrace/btrace   BTRACE_OPTS_VAR JAVA_OPTS The name of the shell variable where the agent parameters should be injected.    Configuration  CONSUL_PATH defines the root of the subtree where the configuration are downloaded from. The root could also contain a configuration config.ini. Default is conf\n CONSUL_KEY is optional. It defines a subdirectory to download the the config files. If both CONSUL_PATH and CONSUL_KEY are defined, the config files will be downloaded from $CONSUL_PATH/$CONSUL_KEY but the config file will be read from $CONSUL_PATH/config.ini\n  "
},
{
	"uri": "https://flokkr.github.io/docs/runtime/swarm/",
	"title": "Docker Swarm",
	"tags": [],
	"description": "",
	"content": " The https://github.com/flokkr/runtime-swarm repository contains example configuration and docker definition to run the flokkr docker images (Hadoop/Spark/Hive/\u0026hellip;) on docker swarm cluster.\nGetting started After creating a docker swarm cluster (with docker swarm init and docker swarm join).\nChoose a subdirectory and deploy it:\ncd ozone docker stack deploy -c docker-compose.yaml ozone  Check the stacks/services:\ndocker stack ls docker stack ps ozone docker service ls  Scale:\ndocker service scale datanode=10  Attributes    Topic Solution     Configuration management continue    Source of config files: docker-compose file (or external env file)   Configuration preprocessing: converted from environment variable with envtoconf   Automatic restart on config change: No   Provisioning and scheduling    Multihost support Yes. Via Swarm networking.   Requirements on the hosts Docker (with initialized swarm mode)   Definition of the containers per host docker-compose deployment section   Scheduling (find hosts with available resource) docker-compose based (global/replicated)   Failover on host crash N/A   Scale up/down: docker service scale service_name=n   Multi tenancy (multiple cluster) Yes (docker swarm networking)   Network    Network between containers Host network   DNS Partial, no reverse dns for the replicated containers.   Service discovery DNS based   Data locality No   Availability of the ports Every publised port is avaiblale on ever node    "
},
{
	"uri": "https://flokkr.github.io/docs/runtime/compose/",
	"title": "Docker compose",
	"tags": [],
	"description": "",
	"content": " The https://github.com/flokkr/runtime-compose repository contains example configuration to run various type of clusters (eg. Hadoop HA, Hadoop federation, Spark, etc.) with docker-compose.\nUsually the could be started with\ndocker-compose up -d  To scale services you can run\ndocker-compose scale datanode=1  But please note that not all the containers can be scaled up. The master components (such as Hadoop namenode) usually have hardcoded hostnames which avoid the scaling,\nCommon properties    Topic Solution     Configuration management    Source of config files: docker-compose external environment variable file   Configuration preprocessing: envtoconf (Convert environment variables to configuration formats   Automatic restart on config change: Not supported, docker-compose up is required   Provisioning and scheduling    Multihost support NO   Requirements on the hosts docker daemon and docker-compose   Definition of the containers per host N/A, one docker-compose file for the local host   Scheduling (find hosts with available resource) NO, localhost only   Failover on host crash NO   Scale up/down: Easy with docker-compose scale datanode=3   Multi tenancy (multiple cluster) Partial (from multiple checkout directory, after port adjustment)   Network    Network between containers dedicated network per docker-compose file   DNS YES, handled by the docker network   Service discovery NO (DNS based)   Data locality NO   Availability of the ports Published according to the docker-compose files    "
},
{
	"uri": "https://flokkr.github.io/docs/",
	"title": "Docker images for Open Source bigdata/hadoop projects",
	"tags": [],
	"description": "",
	"content": " Flokkr is an umbrella github organization to collect all of my containerization work for Apache bigdata/datascience projects such as Apache Hadoop or Apache Spark.\nOn high level, there are two main type of the subprojects/git repos under this organization: Containers and runtime configuration examples.\nIf you would like to run a simple Apache bigdata project, open the repository and use the included docker-compose file. If you need a more sophisticated cluster which includes multiple product and different configuration: investigate the runtime repositories and choose a method which is the most appropriate for you.\nContainers All of the containers are based on one smart baseimage defined in flokkr/docker-baseimage. It contains all the configuration loading script (based on environment variables or consul servers) and other extensions (eg. btrace instrumentation).\nTo get more information about the available environment variables check the flokkr/launcher repository.\nAll the other containers can be found with docker- prefix under the flokkr organization.\nThe containers are usually built on travis-ci and pushed to the docker hub instead to use dockerhub automatic buidls due to the limitation of the dockerhub (for example it\u0026rsquo;s hard to generate matrix builds with all the older versions).\nAvailable images:\n   Repository Product     docker-baseimage Base image with all the configuration loading magic   docker-hadoop Apache Hadoop components (hdfs/yarn)   docker-spark Apache Spark components   docker-storm Apache Storm components   docker-zookeeper Apache Zookeeper components   docker-kafka Apache Kafka components   docker-hbase Apache HBase components   docker-zeppelin Apache Zeppelin interface   docker-krb5 Highly insecure kerberos container, with an open REST api to request new kerberos keytab files.    Note: previous version of the containers (and some not yet migrated) can be found under the github.com/elek account.\nRuntime examples Docker image creation is easy, just a few lines to download and unpack the Apache projects. The tricky part is how the containers could work together: service discovery, configuration management, data locality, multi-tenancy, etc.\nThere are various examples how the containers could be used and each of them have a separated repository with the runtime- prefix.\n   Repository Details     runtime-compose docker-composed based pseudo clusters (multiple containers but only for one hosts). Configuration are defined by environment variables. For development and local experiments.   runtime-consul Multi-host real cluster with consul (for storing the configuration and docker-compose definitions) and docker-compose. Small scripts help to maintain the cluster state (restart components on every config change). Full data-locality is achieved by using docker host network.   runtime-nomad Multi-host real cluster with consul (for storing the configuration and docker-compose definitions) and nomad (to start the instances). Small scripts help to maintain the cluster state (restart components on every config change). Full data-locality is achieved by using docker host network.   runtime-swarm Similar to the previous one, but the container scheduling part is simplified with docker-compose + swarm. No host network, so no data-locality. Environment variable based configuration management.   runtime-kubernetes Kubernetes managed cluster with kubernetes ConfigMap based configuration set.    "
},
{
	"uri": "https://flokkr.github.io/docs/launcher/",
	"title": "Flokkr launcher",
	"tags": [],
	"description": "",
	"content": "The base docker images (and after that every docker image) contains a simple script to launch the command (docker entrypoint).\nThe source of this launcher script is maintained in the launcher repository.\nThe launcher scripts countains multiple conditional script fragment which could be activated by environment variables. These fragments provide additional functionality to the base image. For example:\n Could convert environment variables to configuration files Could instrument java applcation with btrace agent Could retry the execution Could read configuration from consul server Could sleep for a specific time  "
},
{
	"uri": "https://flokkr.github.io/docs/launcher/installer/",
	"title": "Installer plugin",
	"tags": [],
	"description": "",
	"content": "Installer plugin could replace built in components\nThe original products usually unpacked to the /opt directory during the container build (eg. /opt/hadoop, /opt/spark, etc\u0026hellip;). The install plugin deletes the original product directory and replaces it with a newly one downloaded from the internet.\n   Name Default Description     INSTALLER_XXX  The value of the environment variable should be an url. If set, the URL will be downloaded and untar-ed to the /opt/xxx directory. For example set INSTALER_HADOOP=http://home.apache.org/~shv/hadoop-2.7.4-RC0/https://home.apache.org/~shv/hadoop-2.7.4-RC0/hadoop-2.7.4-RC0.tar.gz to test an RC.    "
},
{
	"uri": "https://flokkr.github.io/docs/launcher/kerberos/",
	"title": "Kerberos plugin",
	"tags": [],
	"description": "",
	"content": " Kerberos plugin downloads/generates kerberos keytabs and ssl key/truststore\nOur total UNSECURE kerberos server contains a REST endpoint to download on-the-fly generated kerberos keytabs, java keystores (ssl keystores, trustores). This plugin could be configured to download the files. The plugin also copies krb5.cfg to /etc.\nConfiguration    Name Default Description     KERBEROS_SERVER krb5 The name of the UNSECURE kerberos server where the REST endpoint is available on :8081   KERBEROS_KEYTABS  Space separated list of keytab names. With every element a new keytab will generated to $CONF_DIR/$NAME.keytab with a key for $NAME/$HOSTNAME@EXAMPLE.COM.   KERBEROS_KEYSTORES  Space separated list of certificate names. For every name a new keystore file will be generated to the $CONF_DIR/$NAME.keystore which contains a key for cn=$NAME. Trust store will also be generated to $CONF_DIR/truststore.    "
},
{
	"uri": "https://flokkr.github.io/docs/runtime/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": " The https://github.com/flokkr/runtime-kubernetes contains example resource definitions to run flokkr clusters in kubernetes. These are raw resources usually you can use helm charts instead of the raw resource files: https://github.com/flokkr/charts\nGetting started You need a kubernetes cluster. The easiest way is just using a provider (eg. GCE)\nConfiguration Upload the predefined configuration with:\nkubectl apply -f config.yaml  HDFS/YARN cluster kubectl apply -f hdfs.yaml  Attributes    Topic Solution     Configuration management    Source of config files: Kubernetes configuration object.   Configuration preprocessing: No.   Automatic restart on config change: No.   Provisioning and scheduling    Multihost support Yes.   Requirements on the hosts Full kubernetes cluster.   Definition of the containers per host Kubernetes resource definition.   Scheduling (find hosts with available resource) Yes. But because the kubernetes limitations, only Stateful Sets are used.   Failover on host crash Yes   Scale up/down: Yes, with kubernetes.   Multi tenancy (multiple cluster) Yes.   Network    Network between containers Kubernetes network.   DNS Yes, kubedns based.   Service discovery Based on dns.   Data locality N/A   Availability of the ports Ingress definition is needed.    "
},
{
	"uri": "https://flokkr.github.io/docs/runtime/nomad/",
	"title": "Nomad",
	"tags": [],
	"description": "",
	"content": " The https://github.com/flokkr/runtime-nomad contains an example to use custom dockerized clusters based on the flokkr docker images.\nThis is a custom solution where the configuration management is solved with consul and custom upload/download scripts. But it highly flexible, for example all the components could restart automatically in case of any configuration change.\nGetting started You need a cluster. use terraform or any other tool to start it.\nPrerequisits  Install docker to the nodes Install consul to every node Install nomad to the nodes Install sigil (http://github.com/gliderlabs/sigil)  Configuration Configuration is stored in consul and downloaded by every docker image.\nFirst you need a configuration set:\ngit clone https://github.com/flokkr/configuration.git  You should install the configuration upload (which uploads the configuration with additional preprcessing:\ngo get github.com/flokkr/consync  And now you can upload the configuration:\nconsync -dir ~/projects/flokkr/configuration -consul node-1 -discovery consul  Where node-1 is the hostname of a consul node\nNomad Now you can start the images with\nexport NOMAD_ADDR=http://node-1:4646  And finally:\n./apply.sh namenode.nomad ...  Note: apply.sh is a short script to do client side templating using the sigil utility.\nAttributes    Topic Solution     Configuration management    Source of config files: Consul server (envtoconf can\u0026rsquo;t be used as nomad can\u0026rsquo;t use non-regular env variables)   Configuration preprocessing: consync (Upload configuration to the consul server)   Automatic restart on config change: Supported by the consul plugin of the baseimage. Listens on changes on the consul side   Provisioning and scheduling    Multihost support Yes. Nomad agents should run everywhere   Requirements on the hosts Consul and Nomad agents   Definition of the containers per host Nomad job descriptors   Scheduling (find hosts with available resource) Yes, fine-grained constraint rules   Failover on host crash N/A   Scale up/down: With mondifying the Nomad job specifications   Multi tenancy (multiple cluster) NO (Host network)   Network    Network between containers Host network   DNS Yes, handled by the OS/Cloud environment (Host network)   Service discovery Required for flexible scheduling (Consul based)   Data locality Yes, Full   Availability of the ports All of them are available (Host network)    "
},
{
	"uri": "https://flokkr.github.io/docs/launcher/retry/",
	"title": "Retry plugin",
	"tags": [],
	"description": "",
	"content": " The plugin tries to run the entrypoint of the image multiple times. If the process has been exited with non zero exit code, it tries to rerun the command after a sleep. The sleep time increasing with every iteration and the whole process will be stopped anyway after a fix amount of retry. If the process run enough time (60s) the failure counter and sleep time is reseted.\nConfiguration    Name Default Description     RETRY_NUMBER 10 Number of times the process will be restarted (in case of non-zero exit code   RETRY_NORMAL_RUN_DURATION 60 After this amount of seconds the RETRY_NUMBER counter will be reseted. Example: After 5 tries the process is started and run successfully 5 minutes. After a non-zer exit, it will be rerun RETRY_NUM (10) times. Example 2: After 5 tries the process is starts, runs for 40 seconds, and exits. The retry will continue with the reamining 5 try.    "
},
{
	"uri": "https://flokkr.github.io/docs/runtime/",
	"title": "Runtime environments",
	"tags": [],
	"description": "",
	"content": "The containers could be run in different way. But the end of the day the same questions should be answered:\n How he containers are configured? How can they locate the master components? How they are defined?  Therefore for each runtime example we include a generic descriptor table to help the comparison of various solutions.\n"
},
{
	"uri": "https://flokkr.github.io/docs/launcher/sleep/",
	"title": "Sleep plugin",
	"tags": [],
	"description": "",
	"content": " SLEEP: sleep for a specified amount of time.    Name Default Description     SLEEP_SECONDS  If set, the sleep bash command will be called with the value of the environment variable. Better to not use this plugin, if possible.    "
},
{
	"uri": "https://flokkr.github.io/docs/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]